在显示生活中，许多重要的应用都要处理大规模的流数据。我们的工作目标是需要运行在数十到数百台机器上的应用程序能够在几秒内响应。一些例子如下：

- **网站活动统计**：`Facebook`建了一个分布式聚合系统，名为`Puma`, 这个系统用于为广告商提供过去10-30秒内用户点击它们网页的统计信息，它每秒处理100万个事件。
- **集群监控**：数据中心操作者时常收集程序日志用来发现问题，例如使用上百个节点的`Flume`系统。
- **垃圾探测**：一个社交网络例如`Twitter`可能希望利用统计学学习算法【102】实时识别一个新的垃圾活动。

对于这些应用，我们相信`D-Streams`的延迟在0.5-2秒是可以满足要求的，因为它远低于趋势监控的时间刻度。我们的目标不是需要几百毫秒延迟的应用程序，例如高频率交易。

# 目标

为了在大规模的范围上运行这些应用，我们寻找一个系统设计满足4个目标:

- 1 可扩展到几百个节点
- 2 在基本操作的基础上最小化花费-例如，我们不希望两个副本。
- 3 秒级别的延迟
- 4 从错误和`stragglers`中秒级别的恢复

在我们的认知中，之前的系统不能达到这个目的，副本系统花费太高，向上回退会花费数十秒来恢复丢失的状态。并且它们不能处理`stragglers`。

# 之前的处理模型

虽然在分布式流处理中做了很多工作，但是大部分之前的系统都使用了相同的连续操作模型。在这样的模型下，流计算被划分为一组长期存在的有状态的算子。各算子处理接收的数据并更新它们的内部状态（(比如一个统计某个时间段内页面浏览次数的表）
，然后返回新的记录【29】。图4.1.a描述了这个过程。

![4.1.a](../images/4.1.a.png "4.1.a")

上图即连续操作处理模型。每个节点连续地接收数据、更新内部状态并且输出新的记录。容错一般来说是通过复制数据来实现的，用类似于 `Flux` 或 `DPC`【93, 18】的同步协议来确保副本数据在每个节点看来都是相同的顺序(例如, 当它们有多
个父节点时)。




