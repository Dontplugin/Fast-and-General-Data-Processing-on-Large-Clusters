# 3.4 实现

`Shark` 在 `Spark` 上执行 `SQL`查询使用了与传统的`RDBMS` 类似的三步过程：查询解析，生成逻辑计划和生成物理计划。

对于一个给定的查询，`Shark` 使用 `Apache Hive` 查询编译器来解析该查询并生成抽象语法树。然后语法树被转换成一个逻辑计划，并对该计划进行一些基本的逻辑优化，如采用谓词下推。
到目前为止，`Shark` 和 `Hive` 都采用相同的方法。`Hive` 会将操作转换成由多个`MapReduce` 阶段组成的物理计划。至于 `Shark`，它的优化器采用额外的基于规则的优化，如下推 `LIMIT`
到各个分区，并创建一个由 `RDDs` 转换，而不是 `MapReduce` 任务组成的物理计划。我们可以使用许多 `Spark` 上已有的操作，例如 `map` 和 `reduce`，也可以使用一些为 `Shark` 定制的操作，如
`broadcast joins`（广播连接）。`Spark` 的 `master` 使用标准的 `DAG` 调度技术执行这个依赖图，如使`task` 尽量靠近他们的输入数据，重新运行丢失的任务，以及慢节点任务（`straggler`）迁移等（第
2.5.1 节）。

虽然这种基本方法能够在 `Spark` 上运行 `SQL`，但是让它更高效的执行仍然是具有挑战性的。在 `Shark` 中普遍存在 `UDF` 和复杂的解析函数，使它难以在编译时确定一个最优的查询计划，尤其是
对于那些没有经过 `ETL` 处理的新数据。此外，即使采用这样的方案，直接在 `RDDs` 上执行它也可能是低效的。在本节中，我们将讨论在 `RDD` 模型中高效运行 `SQL` 的优化方法。

# 3.4.1 列式内存存储

内存中数据的表示既影响空间占用又影响读取吞吐量。一个原始的方法是将磁盘中的数据简单地按照原格式缓存，然后在查询处理器中根据需求执行反序列化。这个反序列化成为了很大
的瓶颈：在我们的研究中，我们看到现代的商业`CPUs` 单核的序列化速率仅仅在 200`MB`/秒。

`Spark` 内存存储的默认方式是将数据分区作为 `JVM` 对象集合存储。由于查询处理器能直接使用这些对象，这可以避免反序列化，但会导致严重的存储空间代价。一般的 `JVM` 实现会使得每
个对象增加12到16字节的开销。例如，存储270`MB`的`JVM`对象的`TPC-H` 线项表大约要使用971`MB`的内存，而序列化表示则仅需 289`MB`，存储空间将近为原来的 1/3。但是，更重要的一个影响是垃圾收集
（`Garbage Collection, GC`）。在一个记录大小为 200 字节情况下，一个32`GB` 的堆栈能容纳16 亿的对象。`JVM` 的垃圾收集耗时与堆栈中对象的数量呈线性相关，因此在一个大堆栈上
执行一次完整的垃圾收集（`GC`）可能需要几分钟时间。这些不可预测的、昂贵的垃圾收集导致响应时间会有大的波动。

`Shark` 将基本类型的所有列以 `JVM` 的原始数组形式存储。`Hive` 支持的复杂数据类型，例如 `map`和数组，通过序列化串接成一个单字节数组。每一列仅创建一个 `JVM` 对象，可以带来快速的 `GCs`
和紧凑的数据表示。通过廉价的几乎不需要 `CPU` 成本的压缩技术，可以将列数据的空间占用进一步减少。与列式数据库系统例如`C-store`【100】类似，`Shark` 实现了高效的
`CPU` 压缩模式，例如字典编码、游程编码以及位填充。

列式数据表示可以带来更好的缓存行为，特别是对那些在特定列上频繁进行聚合计算的分析查询。

# 3.4.2 数据协同分区


在一些数据仓库工作中，两个表经常用来进行连接操作。例如，`TPC-H` 基准测试频繁地对`lineitem` 表和 `orders` 表进行连接操作。`MPP` 数据库常用的技术是在数据加载过程中基于两个表
的连接键进行协同分区。在 `HDFS` 等分布式文件系统中，因其存储系统是架构无关的，从而无法进行数据协同分区。`Shark` 允许两个表基于公共键进行协同分区，从而可以在后续的查询中提供
快速的连接操作。它在表的创建声明中增加了 `DISTRIBUTE BY` 语法，用来指定对某个列进行分区。

# 3.4.3 分区统计和映射修剪

通常情况下，数据是在一个或多个列上使用某种逻辑聚类进行存储的。例如，一个网站的流量日志数据项可能基于用户的物理位置信息进行分组的，因为日志首先是被存储在最接近用户
的地理位置的数据中心。在每一个数据中心内，日志只能被添加，并且按照大致的时间顺序进行存储。有一个不太明显的例子，一个新闻网站的数据可能包含具有很强相关性的新闻 `ID`
（`news_id`）和时间（`timestamp`）列。对于分析查询而言，对这些列进行过滤和聚合操作是很典型的，例如，搜索特定时间段或者新闻标题的数据。

映射修剪是基于其自然聚合列对数据进行分区修剪的过程。由于 `Shark` 的内存存储将数据拆分成小的分区，每一个数据块在这些列上只包含一个或几个逻辑组，当数据块落在查询过滤
条件外时，`Shark` 就不需要扫描这些块的数据。

为了利用列（`column`）在自然聚合的优势，`Shark` 在每一个工作节点上的的内存存储会在数据加载过程中附带收集统计信息。每个分区收集来的统计信息包含了每一个列的范围，当不同
的值个数较少的时候，就会包含所有这些不相同的值(例如, 列枚举)。所收集到的统计信息会被发送回驱动节点并存储在内存中，在查询的执行过程中用于修剪分区。当发出一个查询后，`Shark`
会针对查询的目标评估所有的分区统计信息，然后修剪掉没有匹配到目标的分区。这是通过简单地创建一个只依赖于部分父分区的 `RDD` 来实现的。

我们从一个视频分析公司收集了一些基于 `Hive` 仓库的查询样本，在我们收集到的 3833 个查询中，`Shark` 可以利用至少3277个查询包含的谓语来进行映射修剪。章节 3.5 中会提供相
应工作的更多细节。

# 3.4.4 局部 `DAG` 执行 （`PDE`）

像 `Shark` 和 `Hive` 这样的系统经常用来查询未经历过一个数据加载过程的新数据。这就排除了那些依赖精确的数据统计的静态查询优化技术的使用，例如通过索引维护的统计信息。新数据统计信息的
缺乏，再加上 `UDF` 的普遍使用，这就需要动态方法来进行查询优化。

为了在分布式环境中支持动态查询优化，我们扩展了 `Spark` 以支持局部 `DAG` 执行 (`PDE`)，这是一项能够在运行时收集数据的统计信息动态地更改查询计划的技术。

目前，我们将局部 `DAG` 执行应用在阻断“`shuffle`”操作的边界上，在这个阶段数据被交换和重新分区。在 `Shark` 中，这些都是典型的消耗最大的操作。默认情况下 `Spark` 在每次 `shuffle`
前都会将 `map` 任务的结果物化到内存中，必要时会溢出到磁盘中。然后，`reduce` 任务会获取这些输出。

`PDE` 从两个途径上修改了此机制。首先，它在全局上收集可定制的统计信息，以及物化 `map` 任务输出时每个分区的粒度。其次，它允许基于这些统计信息来更改 `DAG`，或通过选择不同的操
作，或改变其参数（例如他们的并行度）。

这些统计数据可以通过使用简单的可插拔的累加器 `API` 来自定义。一些示例统计信息包括：

　　1. 分区大小和记录数目，可用于数据倾斜检测。
　　2. “重量级”列表，也就是说,那些经常出现在数据集中的数据项。
　　3. 近似直方图，可以被用来估计分区的数据布局。

这些统计信息由每个 `worker` 传送给 `master`，然后它们在那里汇总并提交给优化器。为了提高效率，我们采用有损压缩来记录统计信息，限制其大小为每个任务 1-2`kB`。例如，我们将分区
的大小（以字节为单位）用对数编码来实现，可以只用一个字节来表示高达 32`GB` 的大小，这样的误差最高也只有 10%。然后，`master` 可以使用这些统计信息来执行各种多样的运行优化，
我们将在接下来讨论这些。

当前使用`PDE` 已经实现的优化的例子包括：
• `Join` 算法选择。当两个表进行连接时，`Shark `使用 `PDE` 来选择运行 `shuffle` 连接（对两个集合的记录在全网对它们的  `key ` 进行哈希）还是广播连接（将较小表广播到所有节点）。优
化算法取决于表的大小：如果一个表比其他小很多，广播连接就会使用较少的网络通信。因为表的大小可能不会被事先知道 (例如,由  `UDF` 引起的), 在运行时选择算法能做出更好的决策。

• 并行度。 `reduce` 任务的并行度在类 `MapReduce` 的系统中有较大的性能影响：启动太少`reducer` 任务会使得 `reducer` 的网络连接过载，并消耗完它们的内存，而如果启动太多的
话可能会由于调度开销延长作业【17】。基于局部 `DAG` 执行，`Shark` 可以使用单个分区的大小决定运行时 `reducer` 的数目。通过将许多小的细粒度的分区合并为较少的粗粒度分区，
来减少 `reduce` 任务的数目。

• 倾斜处理。以类似的方式，通过将 `map` 任务的结果预先划分成许多小块，可以帮助我们来选择 `reduce` 任务的数目，也可以帮助我们识别并特别处理一些特殊的热键。这些活跃
的分区可以由单独的 `reduce` 任务来处理，那些其他块则可以合并起来形成较大的任务。

局部 `DAG` 执行实现了现有的一些在单节点系统中典型的自适应查询优化技术，【16, 63,107】,因为我们可以使用现有的技术动态地优化每个节点内的本地计划 ，并在阶段的边界使用
`PDE` 来优化执行计划的全局结构。细粒度的统计信息收集以及优化，使 `PDE` 区别于先前系统（例如 `DryadLINQ` 【115】）中的图重写特性。

而 `PDE` 目前仅在我们的 `Shark` 原型中实现，未来我们计划将其添加到 `Spark` 的核心（`Core`）引擎中去，从而从这些优化中收益。

