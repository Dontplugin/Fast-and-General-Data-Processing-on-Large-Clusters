# 3.4 Implementation

Shark 在 Spark 上执行 SQL 查询的步骤与传统的 RDBMS 类似：查询解析，生成逻辑计划和生
成物理计划的。
对于一个给定的查询，Shark 使用 Apache Hive 查询编译器来解析该查询并生成抽象语法树。
然后语法树被转换成一个逻辑计划，并对该计划进行一些基本的逻辑优化，如采用谓词下推
（pushdown）。到目前为止，Shark 和 Hive 都采用相同的方法。Hive 会将操作转换成由多个
MapReduce 阶段组成的物理计划。至于 Shark，它的优化器采用额外的规则优化，如推送 LIMIT
到各个分区，并创建一个由 RDDs 转换，而不是 MapReduce 任务组成的物理计划。我们可以使
用许多 Spark 上已有的操作，例如 map 和 reduce，也可以使用一些为 Shark 定制的操作，如
broadcast joins（广播连接）。Spark 的 master 使用标准的 DAG 调度技术执行这个依赖图，如
将 task 尽量保证数据本地性，重新运行丢失的任务，以及慢节点任务（straggler）迁移等（第
2.5.1 节）。
虽然这种基本方法能够在 Spark 上运行 SQL，但是让它更高效的执行仍然是具有挑战性的。
在 Shark 中普遍存在 UDF 和复杂的解析函数，使它难以在编译时确定最优的查询计划，尤其是
对于那些没有经过 ETL 处理的新数据。此外，即使采用这样的方案，直接在 RDDs 上执行它也可
能是低效的。在本节中，我们将讨论在 RDD 模型中高效运行 SQL 的优化方法。

# 3.4.1 Columnar Memory Store

列式内存存储
内存中数据的表示既影响空间占用又影响读取吞吐量。一个原始的方法是简单的将磁盘中
的数据按照原格式缓存，然后在查询处理中根据需求执行反序列化。这个反序列化成为了很大
的瓶颈：在我们的研究中，我们看到现代的商业 CPUs 单核的序列化速率仅仅在 200MB/秒。
Spark 内存存储的默认方式是将数据分区作为 JVM 对象集合存储。由于查询处理器能直接使
用这些对象，这可以避免反序列化，但会付出严重的存储空间代价。一般的 JVM 实现会使得每
个对象增加12到16字节的开销。例如，存储270MB的JVM对象的TPC-H 线项表大约要使用971MB
的内存，而序列化表示则仅需 289MB，存储空间将近为原来的 1/3。但是，更重要的是垃圾收集
（Garbage Collection, GC）的影响。在一个记录大小为 200 字节情况下，32GB 的堆栈能容纳
16 亿的对象。JVM 的垃圾收集耗时与堆栈中对象的数量呈线性相关关系，因此在一个大堆栈上
执行一次完整的垃圾收集（GC）可能需要几分钟时间。这些不可预测的、昂贵的垃圾收集导致
响应时间会有大的波动。
Shark 将基本类型的列以 JVM 的原始数组形式存储。Hive 支持的复杂数据类型，例如 map
和数组，通过序列化串接成一个字节数组。每一列仅创建一个 JVM 对象，可以带来快速的 GCs
和紧凑的数据表示。通过廉价的压缩技术，这种压缩技术基本上不需要 CPU 成本，可以将列数
据的空间占用进一步减少。与列数据库系统类似，e.g., C-store [100]，Shark 实现了高效的
CPU 压缩效率模式，例如字典编码、游程编码以及位填充。
列式数据表示可以带来更好的缓存行为，特别是对那些频繁在特定列上进行聚合计算的分
析查询。

# 3.4.2 Data Co-partitioning

数据协同划分
在一些数据仓库工作中，两个表经常用来进行连接操作。例如，TPC-H 基准测试频繁地对
lineitem 表和 orders 表进行连接操作。MPP 数据库常用的技术是在数据加载过程中基于两个表
的连接键进行协同划分。在 HDFS 等分布式文件系统中，因其存储系统是架构无关的，从而无法
进行数据协同划分。Shark 允许两个表基于公共键进行共同划分，这可以在后续的查询中提供
快速的连接操作。它在表的创建声明中增加了 DISTRIBUTE BY 语法，用来指定对某个列进行划
分。

# 3.4.3 Partition Statistics and Map Pruning

分区统计和映射修剪
通常情况下，数据是在一个或多个列上使用某种逻辑聚合进行存储的。例如，一个网站的
流量日志数据项可能基于用户的物理位置信息进行分组的，因为日志首先是被存储在距离用户
最佳地理位置的数据中心。在每一个数据中心内，日志只能被添加，并且按照大致时间顺序进
行存储。作为一个不太明显的例子，一个新闻网站的数据可能包含具有很强相关性的新闻 ID
（news_id）和时间（timestamp）列。对于分析查询，对这些列进行过滤和聚合操作是很典型
的，例如，搜索特定时间段或者新闻标题的有关数据。
映射修剪是基于其自然聚合列对数据进行分区修剪的过程。由于 Shark 的内存存储将数据
拆分成小的分区，每一个数据块在这些列上只包含一个或几个逻辑组，当数据块落在查询过滤
条件外时，Shark 可以不用进行数据扫描。
为了利用列（column）在自然聚合的优势，Shark 在每一个工作节点上的的内存存储会在数
据加载过程中附带收集统计信息。每个分区收集来的统计信息包含了每一个列的范围，当不同
的值个数较少的时候，会包含所有不相同的值(例如, 枚举列)。所收集到的统计信息会被发送
回驱动节点并存储在内存中，在查询的执行过程中用于修剪分区。当发出一个查询后，Shark
会针对查询的目标评估所有的分区统计信息，然后修剪掉那些没有匹配目标的分区。这是通过
简单地创建只依赖于一些父分区的 RDD 来实现的。
我们从一个视频分析公司收集了一些基于 Hive 仓库的查询样本，在我们收集到的 3833 个查
询中，至少有 3277 个 Shark 可以利用他们所包含的谓语来进行映射修剪。章节 3.5 中会提供相
应工作的更多细节。

# 3.4.4 Partial DAG Execution (PDE)

局部 DAG 执行 （PDE）
像 Shark 和 Hive 系统经常用来查询未经历过一个数据加载过程的新数据。这就排除了那些
依赖精确数据统计的静态查询优化技术的使用，例如通过索引维持的统计信息。新数据统计的
缺乏，再加上 UFD 的普遍使用，这就需要动态方法来进行查询优化。
在分布式环境中支持动态查询优化，我们扩展了 Spark 以支持局部 DAG 执行 (PDE)，这是
一项能够允许运行时收集数据统计信息进行动态地改变查询计划的技术。
目前，我们将局部 DAG 执行应用在阻断“shuffle”操作的边界上，在这个阶段数据被交换
和重新划分。在 Shark 中，这些都是典型的消耗最大的操作。默认情况下 Spark 在每次 shuffle
前都会将 map 任务的结果物化到内存中，必要时会溢出到磁盘中。然后，reduce 任务会获取这
些输出。

PDE 从两个方面修改了此机制。首先，它在全局上收集可定制的统计信息，以及物化 map 任
务输出时每个分区的粒度。其次，它允许基于这些统计信息来改变 DAG，或通过选择不同的操
作，或改变其参数（例如他们的并行度）。
这些统计数据可以通过使用简单的可插拔累加器 API 来自定义。一些例子的统计信息包括：
1. 分区大小和记录计数，可用于数据倾斜检测。
2. “重量级”列表，也就是说,那些经常出现在数据集中的项目。
3. 近似直方图，可以被用来估计分区的数据布局。
这些统计信息由每个 worker 传送给 master，然后它们在那里汇总并提交给优化器。为了提
高效率，我们采用有损压缩记录统计信息，限制其大小为每个任务 1-2kB。例如，我们将分区
的大小（以字节为单位）用对数编码来实现，可以只用一个字节来表示高达 32GB 的大小，这样
误差最高也只有 10%。然后，master 可以使用这些统计信息来执行各种多样的运行优化，这些
我们将接下来讨论。
使用当前在 PDE 中已经实现的优化的例子包括：
• Join 算法选择。当连接两个表时，Shark 使用 PDE 来选择运行 shuffle 连接（对两个集
合的记录在全网对它们的 key 进行哈希）还是广播连接（将较小表广播到所有节点）。优
化算法取决于表的大小：如果一个表比其他的小很多，广播连接就会使用较小的网络通
信。因为表的大小可能不会被事先知道 (例如,由 UDF 引起的), 在运行时选择算法会做
出更好的决策。
• 并行度。reduce 任务的并行度在类 MapReduce 的系统中有较大的性能影响：启动太少
reduce 任务会使得 reducer 的网络连接过载，并消耗完它们的内存，而如果启动太多的
话可能会由于调度开销延长作业[17]。基于局部 DAG 执行，Shark 可以使用单个分区的大
小决定运行时 reduce 任务的数目。通过将许多小的细粒度的分区合并为粗粒度的分区，
来减少 reduce 任务的数目。
• 倾斜处理。以类似的方式，通过将 map 任务的结果预先划分成许多小块，可以帮助我们
来选择 reduce 任务的数目，也可以帮助我们识别并特别处理一些特殊的 key。这些特殊
的分区可以由单独的 reduce 任务来处理，那些其他块则可以合并起来形成较大的任务。
局部 DGA 执行实现了现有的一些在单机系统中典型的自适应查询优化技术，[16, 63,107],
因为我们可以使用现有的技术，来动态地优化每个节点内的本地计划 ，并在阶段的边界使用
PDE 来优化执行计划的全局结构。细粒度的统计信息收集以及优化，使 PDE 区别于先前系统例
如 DryadLINQ [115]中的图重写特性。
而 PDE 目前仅在我们的 Shark 原型中实现，未来我们计划将其添加到 Spark 的核心（Core）
引擎中去，从而从这些优化中收益。

